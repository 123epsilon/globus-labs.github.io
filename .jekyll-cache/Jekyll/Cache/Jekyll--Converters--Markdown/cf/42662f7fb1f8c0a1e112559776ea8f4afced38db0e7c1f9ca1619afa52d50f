I"A<p>A preprint of our first paper on ScholarBERT “<a href="https://arxiv.org/abs/2205.11342">ScholarBERT: Bigger is not always better</a>” by Hong, Aswathy and Greg et al is now published on arXiv.
The pretrained ScholarBERT models are available on <a href="https://huggingface.co/globuslabs">HuggingFace Hub</a>!</p>
:ET